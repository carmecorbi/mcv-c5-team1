import argparse
import os
import re
import torch
import ast

import pandas as pd

from transformers import AutoTokenizer, AutoModelForCausalLM
from diffusers import StableDiffusionPipeline, DDPMScheduler
from tqdm import tqdm

# Define constants
MODEL_ID = "stabilityai/stable-diffusion-2-1"
DEFAULT_NUM_INFERENCE_STEPS = 50
DEFAULT_GUIDANCE_SCALE = 7.5

WIDTH = 274
HEIGHT = 169

# Check if GPU is available and set the device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


def get_image_model():
    # Define the stable diffusion pipeline
    pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16)
    unet_params = sum(p.numel() for p in pipe.unet.parameters() if p.requires_grad)
    print(f"UNet parameters: {unet_params / 1e6:.1f} M")
    text_encoder_params = sum(p.numel() for p in pipe.text_encoder.parameters() if p.requires_grad)
    print(f"Text encoder parameters: {text_encoder_params / 1e6:.1f} M")

    # Load scheduler
    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)
    pipe.to(device)
    return pipe

def generate_img(pipe, prompt, num_steps = DEFAULT_NUM_INFERENCE_STEPS, guidance_scale = DEFAULT_GUIDANCE_SCALE):
    """Generate an image using the Stable Diffusion pipeline.

    Args:
        pipeline (Any): Stable Diffusion pipeline instance
        prompt (str): The prompt to generate the image.
        num_steps (int, optional): Number of steps to generate image. Defaults to 50.
        guidance_scale (float, optional): Guidance scale value. Defaults to 7.5.

    Returns:
        Any: The generated image.
    """
    negative_prompt = "unnatural colors, overly saturated, neon colors, garish colors, unrealistic hues, non-food or non-drink items, floating ingredients, isolated fruit, ingredients out of context, plates within plates, nested plates, multiple plates, duplicate plates, overlapping plates, scattered or disjointed presentation, blurred, low resolution, distracting background elements, text, hand"
    positive_prompt = f"{prompt} dish (food or drink), professional food or drink photography, integrated dish presentation with ingredients arranged on a plate without fork/knife or on a dinnerwear element (e.g., plate, glass, cup), soft natural lighting, shallow depth of field, restaurant setting, focus on the dish, high-resolution, sharp details, minimal background distraction, cohesive composition that ensures all ingredients are integrated into a unified and realistic presentation"
    image = pipe(
        prompt=positive_prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=num_steps,
        guidance_scale=guidance_scale
    ).images[0]
    
    image = image.resize((WIDTH, HEIGHT))
    return image

def clean_filename(text):
    return re.sub(r"[^a-zA-Z0-9_-]", "_", text)[:80]

def save_img(image, prompt: str, output_dir: str):
    """Save the generated image to a file.

    Args:
        image (Any): The generated image. 
        prompt (str): The prompt used to generate the image.
        output_dir (str): The directory where the image will be saved.
    """
    safe_prompt = clean_filename(prompt)
    filename = f"{safe_prompt}.jpg"
    filepath = os.path.join(output_dir, filename)

    # Save the image
    image.save(filepath)
    print(f"Image saved as: {filepath}")


def get_text_model(token: str):
    model_name = "google/gemma-3-1b-it"
    
    # Load the model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        token=token
    )
    return tokenizer, model


def generate_variations(caption: str, tokenizer, model, num_variations: int = 3) -> list[str]:
    """Given a image caption, it should generate multiple variations of the same dish or drink.
    It uses LLM to generate the variations. The variations are generated by modifying the caption slightly. Avoiding complex scenes.
    The variations should be different but still related to the same dish.

    Args:
        caption (str): The caption of the image.
        num_variations (int, optional): Number of variations. Defaults to 3.
        
    Returns:
        list[str]: List of variations.
    """
    # Create a prompt that instructs the model to generate variations
    prompt = f"""Generate {num_variations} different caption variations for the food dish or drink title: "{caption}"

    Requirements:
    - Each variation must be a coherent, dish-specific title (not just a single ingredient) and must contain at least 2 words and no more than 10 words.
    - Each caption must be unique and at least 2 of the variations cannot be a rephrasing of the original caption.
    - Each caption must include at least one specific food or drink ingredient. 
    - Focus on aspects such as key ingredients, preparation methods, or regional styles that contribute to a new version of the dish title.
    - Ensure that each variation describes the same dish, but from distinct angles (e.g., emphasizing texture, flavor profile, or origin).
    - Do NOT use generic phrases like "a photo of" or "a picture of".
    - Do NOT repeat exactly the same caption as in the prompt or duplicate any generated variation.
    - Only produce captions for FOOD DISHES, COCKTAILS, DRINKS, or DESSERTS. Do not generate variations for other types of images.

    For example, if the original is "Horchata", acceptable variations might be:
    Almond Horchata
    Creamy Horchata Blend
    Mexican Horchata

    Variations:
    """
    
    # Encode the prompt
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate response
    with torch.no_grad():
        output = model.generate(
            inputs["input_ids"],
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode the output
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # Extract the generated variations (removing the prompt)
    response_text = generated_text[len(prompt):].strip()
    
    # Split the response into individual variations
    variations = []
    for line in response_text.split("\n"):
        line = line.strip()
        # Skip empty lines or lines that aren't variations
        if line and not line.startswith("Original caption"):
            # Clean up potential numbered formats like "1. " or "1) "
            cleaned_line = line
            if len(line) > 2 and (line[0].isdigit() and line[1] in ['.', ')', '-']):
                cleaned_line = line[2:].strip()
            variations.append(cleaned_line)
    
    # Ensure we have the requested number of variations
    while len(variations) < num_variations and variations:
        raise ValueError(f"Not enough variations generated. Expected {num_variations}, got {len(variations)}.")
    return variations[:num_variations]

def read_data(dataset_path: str):
    data = pd.read_csv(dataset_path)
    data['Title'] = data['Title'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [x] if isinstance(x, str) else x)
    return data

def create_synthetic_dataframe(data_df, tokenizer, model, pipe, variations_per_caption, output_dir):
    """
    Create a synthetic dataframe with generated caption variations and their images.
    
    Args:
        data_df: Original dataframe with Image_Name and Title columns
        tokenizer: Text model tokenizer
        model: Text model
        pipe: Image generation model pipeline
        variations_per_caption: Number of variations to generate per caption
        output_dir: Directory to save the synthetic dataframe and images
        
    Returns:
        Synthetic dataframe with Image_Name and Title columns
    """
    synthetic_data = []
    
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    for _, row in tqdm(data_df.iterrows()):
        # Handle cases where Title might be a list or a string
        captions = row['Title']
        if not isinstance(captions, list):
            captions = [captions]
        
        # Generate variations for each caption
        for caption in captions:
            if isinstance(caption, str):
                variations = generate_variations(caption, tokenizer, model, variations_per_caption)
                
                # Add each variation to the synthetic data and generate images
                print(f"Generating variations for caption: {caption}")
                for i, variation in enumerate(variations):
                    # Create a unique image name based on original image and variation number
                    variation_image_name = f"{variation}_var{i+1}"
                    print(f"Generating image for variation: {variation}")
                    
                    # Generate and save image
                    image = generate_img(pipe, variation)
                    save_img(image, variation_image_name, output_dir)
                    
                    # Add to synthetic data
                    synthetic_data.append({
                        'Image_Name': variation_image_name,
                        'Title': [variation]  # Store as a list to match original format
                    })
                print("-" * 20)
    
    # Create dataframe from the collected synthetic data
    synthetic_df = pd.DataFrame(synthetic_data)
    
    # Save the synthetic dataframe
    synthetic_df.to_csv(os.path.join(output_dir, 'synthetic_captions.csv'), index=False)
    return synthetic_df


# Example usage
if __name__ == "__main__":
    argparser = argparse.ArgumentParser(description="Generate images and variations.")
    argparser.add_argument("--token", type=str, required=True, help="Token for huggingface.")
    argparser.add_argument("--variations", type=int, default=3, help="Number of variations to generate per caption.")
    argparser.add_argument("--data_csv", type=str, default="/ghome/c5mcv01/mcv-c5-team1/week4/data/final.csv", help="CSV where the data is stored.")
    argparser.add_argument("--output_dir", type=str, default="results/synthetic_data", help="Directory to save generated images.")
    argparser.add_argument("--start_idx", type=int, default=0, help="Start index for processing.")
    args = argparser.parse_args()
    
    # Get arguments
    token = args.token
    output_dir = args.output_dir
    data_csv = args.data_csv
    variations_per_caption = args.variations
    
    print(f"Reading data from: {data_csv}")
    print(f"Output directory: {output_dir}")
    print(f"Generating {variations_per_caption} variations per caption")
    
    # Get dataframe
    df = read_data(data_csv)
    df = df.iloc[args.start_idx:]
    print(df.head())
    print(f"Total images to process: {len(df)}")
    
    # Get text model
    tokenizer, model = get_text_model(token)
    
    # Get image model
    pipe = get_image_model()
    
    # Create synthetic dataframe with caption variations and generate images
    synthetic_df = create_synthetic_dataframe(df, tokenizer, model, pipe, variations_per_caption, output_dir)
    print(f"Generated {len(synthetic_df)} caption variations and images")
    print(f"Synthetic dataframe saved to {os.path.join(output_dir, 'synthetic_captions.csv')}")
