<h1 align="center">WEEK 4: Multimodal Recognition (Image Captioning-2)</h1>

# Table of Contents

- [Project Structure W2](#project-structure-w2)

# Project Structure W4

# Task 1: Image Captioning using ViT-GPT2 architecture

## Task 1.1: Direct evaluation using pretrained weights from huggingface model "nlpconnect image-captioning"

## Task 1.2: Fine-tuning strategies

### ViT (Fine-Tune), GPT2 (Frozen)

### ViT (Frozen), GPT2 (Fine-Tune)

### ViT (Fine-Tune), GPT2 (Fine-Tune)

## Task 1.3: Report a single table comparing the above methods using BLEU-1, BLEU-2, ROUGE-L, and METEOR

## Task 1.4: Compare and discuss your results against those obtained using last week's methods

# Task 2: Image Captioning with LLMs

## Task 2.1: Direct evaluation using Llama 3.2-11B model (multimodal)

## Task 2.2: Use your well trained ViT encoder as a frozen image feature extractor, and fine-tune decoders (Llama 3.2-1B and Llama 3.2-3B) using LoRA

## Task 2.3: Report a single table comparing the above methods using BLEU-1, BLEU-2, ROUGE-L, and METEOR

## Task 2.4: Compare and discuss the results obtained from all methods
