from transformers import AutoProcessor, Gemma3ForConditionalGeneration
from transformers import ViTImageProcessor
from src.tokenizer import Tokenizer
from src.dataset.data import Data
from src.metrics.metrics import Metric
from tqdm import tqdm

import pandas as pd

import torch
import argparse
import os

model_id = "google/gemma-3-4b-it"

model = Gemma3ForConditionalGeneration.from_pretrained(
        model_id, device_map="auto"
    ).eval()
processor = AutoProcessor.from_pretrained(model_id, use_fast=True)


def run_model(image_path: str) -> str:
    """Run the model with the given image path.

    Args:
        image_path (str): The path to the image.

    Returns:
        str: The generated caption.
    """
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You have to do image captioning with the images I provide to you. Only do the image captioning as an expert in dishes. \
                        Be as much specific with the dish, only provide the caption, nothing more, nothing less."}]
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image", 
                    "path": image_path
                }
            ]
        }
    ]

    inputs = processor.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=True,
        return_dict=True, return_tensors="pt"
    ).to(model.device, dtype=torch.bfloat16)

    input_len = inputs["input_ids"].shape[-1]

    with torch.inference_mode():
        generation = model.generate(**inputs, max_new_tokens=25, do_sample=False)
        generation = generation[0][input_len:]

    decoded = processor.decode(generation, skip_special_tokens=True)
    return decoded


def evaluate(dataset: torch.utils.data.Dataset, tokenizer: Tokenizer) -> list:
    """
    Evaluate the model over a full torch Dataset.

    Args:
        dataset (torch.utils.data.Dataset): Dataset where each sample is a dict containing:
            - "image_path": str, the path to the image.
            - Optionally "caption": the ground truth caption for later metric computations.

    Returns:
        List[dict]: A list of dictionaries, each containing:
            - "image_path": the image path.
            - "generated_caption": the caption generated by run_model.
            - "ground_truth": the ground truth caption if available.
    """
    metric = Metric()
    
    metrics_sum = {}
    num_samples = 0
    
    for _, caption_idx, image_path in tqdm(dataset):
        # Run the inference model and get the generated caption
        generated_caption = run_model(image_path)
        
        # Get the ground truth caption if available
        ground_truth = tokenizer.decode(caption_idx)
        
        # Get the metrics
        print("Generated caption:", generated_caption)
        print("Ground truth caption:", ground_truth)
        result = metric([generated_caption], [[ground_truth]])
        
        # Accumulate metrics
        for key, value in result.items():
            metrics_sum[key] = metrics_sum.get(key, 0) + value
        num_samples += 1
        
    averaged_metrics = {key: value / num_samples for key, value in metrics_sum.items()}
    return averaged_metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the model with the given image path.")
    parser.add_argument("-t", "--task", type=str, help="The task to perform (infer, eval)", required=True)
    parser.add_argument('--eval_set', help="Evaluation set to use", default="test", choices=["train", "test", "val"])
    parser.add_argument("-i", "--image_path", type=str, help="The path to the input image.", required=False)
    parser.add_argument("-d", "--data_dir", type=str, help="The path to the data directory.", required=False)
    parser.add_argument("-tok", "--tokenizer_path", type=str, help="The path to the tokenizer.", required=False, default="nlpconnect/vit-gpt2-image-captioning")
    args = parser.parse_args()
    
    if args.task == "infer":
        assert args.image_path is not None, "Please provide an image path."
        
        # Run the model with the given image path
        caption = run_model(args.image_path)
        print("Generated caption:", caption)
    elif args.task == "eval":
        assert args.data_dir is not None, "Please provide a data directory."
        
        # Load tokenizer and image processor
        train_csv_path = os.path.join(args.data_dir, "train.csv")
        val_csv_path = os.path.join(args.data_dir, "val.csv")
        test_csv_path = os.path.join(args.data_dir, "test.csv")
        img_path = os.path.join(args.data_dir, "images")
        tokenizer = Tokenizer(tokenizer_path=args.tokenizer_path)
        image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
        
        # Load datasets
        train_df, val_df, test_df = pd.read_csv(train_csv_path), pd.read_csv(val_csv_path), pd.read_csv(test_csv_path)
        partitions = {
            'train': list(train_df.index),
            'val': list(val_df.index),
            'test': list(test_df.index)
        }
        
        # Select the correct dataset based on eval_set
        if args.eval_set == "test":
            data_eval = Data(test_df, partitions['test'], img_path=img_path, tokenizer=tokenizer, image_processor=image_processor, return_path=True)
        elif args.eval_set == "val":
            data_eval = Data(val_df, partitions['val'], img_path=img_path, tokenizer=tokenizer, image_processor=image_processor, return_path=True)
        elif args.eval_set == "train":
            data_eval = Data(train_df, partitions['train'], img_path=img_path, tokenizer=tokenizer, image_processor=image_processor, return_path=True)
        
        # Evaluate the model over a full torch Dataset
        print(evaluate(data_eval, tokenizer))