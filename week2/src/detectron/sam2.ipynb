{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 - Decoder finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/yeray142/Documents/projects/mcv-c5-team1/data\"\n",
    "INSTANCES_PATH = os.path.join(DATA_PATH, \"instances\")\n",
    "TRAINING_PATH = os.path.join(DATA_PATH, \"training\")\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  2002 10000]\n",
      "[ 0  2 10]\n"
     ]
    }
   ],
   "source": [
    "img = np.array(Image.open(\"/home/yeray142/Documents/projects/mcv-c5-team1/data/instances/0000/000000.png\"))\n",
    "obj_ids = np.unique(img)\n",
    "class_ids = obj_ids // 1000\n",
    "\n",
    "print(obj_ids)\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_instance_masks(annotation_path):\n",
    "    \"\"\"\n",
    "    Extracts individual instance masks and their class IDs from a 16-bit PNG annotation file.\n",
    "    \n",
    "    Args:\n",
    "        annotation_path (str): Path to the annotation PNG file.\n",
    "        \n",
    "    Returns:\n",
    "        masks (dict): A dictionary where the keys are `(class_id, instance_id)` tuples,\n",
    "                      and the values are binary masks of shape (H, W).\n",
    "    \"\"\"\n",
    "    # Load the annotation image (16-bit, single-channel)\n",
    "    img = np.array(Image.open(annotation_path))\n",
    "    \n",
    "    # Get unique object IDs\n",
    "    obj_ids = np.unique(img)\n",
    "\n",
    "    # Dictionary to store instance masks\n",
    "    masks = {}\n",
    "    for obj_id in obj_ids:\n",
    "        if obj_id == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract class_id and instance_id\n",
    "        class_id = obj_id // 1000\n",
    "        instance_id = obj_id % 1000\n",
    "\n",
    "        # Generate binary mask for this specific object\n",
    "        mask = (img == obj_id).astype(np.uint8)\n",
    "\n",
    "        # Store mask in dictionary with (class_id, instance_id) as key\n",
    "        masks[(class_id, instance_id)] = mask\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class KITTIMOTSBaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Base dataset class for loading KITTI-MOTS images and instance masks.\n",
    "    \"\"\"\n",
    "    def __init__(self, training_path, instances_path, split=\"train\", transform=None, resize=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            training_path (str): Root directory of training/validation images (TRAINING_PATH)\n",
    "            instances_path (str): Root directory of instance segmentation masks (INSTANCES_PATH)\n",
    "            split (str): Either 'train' or 'val'\n",
    "            transform (callable, optional): Optional transform to apply to images and masks\n",
    "            resize (tuple, optional): If provided, resize all images and masks to this size (width, height)\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.training_path = os.path.join(training_path, split)\n",
    "        self.instances_path = instances_path\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        \n",
    "        # Collect image-mask pairs\n",
    "        self.paired_data = self._match_images_and_masks()\n",
    "        \n",
    "    def _match_images_and_masks(self):\n",
    "        \"\"\"Match images with their corresponding instance masks.\"\"\"\n",
    "        paired_data = []\n",
    "        \n",
    "        # Traverse the training/val directory structure\n",
    "        seq_dirs = [d for d in os.listdir(self.training_path) \n",
    "                    if os.path.isdir(os.path.join(self.training_path, d))]\n",
    "        \n",
    "        for seq_id in seq_dirs:\n",
    "            img_dir = os.path.join(self.training_path, seq_id)\n",
    "            mask_dir = os.path.join(self.instances_path, seq_id)\n",
    "            \n",
    "            # Skip if mask directory doesn't exist\n",
    "            if not os.path.exists(mask_dir):\n",
    "                print(f\"Warning: Mask directory {mask_dir} not found. Skipping sequence {seq_id}.\")\n",
    "                continue\n",
    "            \n",
    "            # Get all image files in this sequence\n",
    "            img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "            \n",
    "            for img_file in img_files:\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                mask_path = os.path.join(mask_dir, img_file)\n",
    "                \n",
    "                # Check if corresponding mask exists\n",
    "                if os.path.exists(mask_path):\n",
    "                    paired_data.append((img_path, mask_path))\n",
    "                else:\n",
    "                    print(f\"Warning: Mask not found for {img_path}\")\n",
    "        \n",
    "        return paired_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paired_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and mask paths\n",
    "        img_path, mask_path = self.paired_data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load mask (16-bit PNG)\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Process the mask to extract instance information\n",
    "        binary_mask = (mask > 0).astype(np.uint8)\n",
    "        binary_mask_pil = Image.fromarray(binary_mask)\n",
    "        \n",
    "        # Apply resize if specified\n",
    "        if self.resize:\n",
    "            image = image.resize(self.resize, Image.BILINEAR)\n",
    "            binary_mask_pil = binary_mask_pil.resize(self.resize, Image.NEAREST)\n",
    "            binary_mask = np.array(binary_mask_pil)\n",
    "        \n",
    "        # Convert to numpy array after potential resize\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Apply additional transforms if any\n",
    "        if self.transform:\n",
    "            image_np, binary_mask = self.transform(image_np, binary_mask)\n",
    "        \n",
    "        return {\n",
    "            \"image\": image_np,\n",
    "            \"label\": binary_mask,\n",
    "            \"image_path\": img_path,\n",
    "            \"mask_path\": mask_path,\n",
    "            \"original_mask\": mask,\n",
    "            \"image_size\": image_np.shape[:2]  # Store original size (H, W)\n",
    "        }\n",
    "\n",
    "def get_bounding_box(mask):\n",
    "    \"\"\"\n",
    "    Calculate the bounding box coordinates for a binary mask.\n",
    "    \n",
    "    Args:\n",
    "        mask (np.ndarray): Binary mask of shape (H, W)\n",
    "        \n",
    "    Returns:\n",
    "        list: Bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "    \"\"\"\n",
    "    # Find rows and columns where mask is True\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    \n",
    "    # Handle empty mask case\n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return [0, 0, 0, 0]\n",
    "    \n",
    "    # Get min/max indices for rows and columns\n",
    "    y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "    x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "    \n",
    "    return [int(x_min), int(y_min), int(x_max), int(y_max)]\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is used to create a dataset that serves input images and masks\n",
    "    formatted specifically for SAM2.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        \n",
    "        # Get bounding box prompt\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        \n",
    "        # Prepare image and prompt for the model\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "        \n",
    "        # Add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "        \n",
    "        # Optionally pass through additional information\n",
    "        inputs[\"image_path\"] = item[\"image_path\"]\n",
    "        inputs[\"mask_path\"] = item[\"mask_path\"]\n",
    "        inputs[\"image_size\"] = item[\"image_size\"]\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that handles variable-sized images and masks.\n",
    "    This function:\n",
    "    1. Processes SAM2-specific tensor inputs (pixel_values, etc.) \n",
    "    2. Pads ground_truth_masks to the same size\n",
    "    3. Collects non-tensor items (paths, etc.)\n",
    "    \"\"\"\n",
    "    # Group data by key\n",
    "    grouped_data = {key: [item[key] for item in batch] for key in batch[0].keys()}\n",
    "    \n",
    "    # Determine max dimensions for padding\n",
    "    if \"ground_truth_mask\" in grouped_data:\n",
    "        masks = grouped_data[\"ground_truth_mask\"]\n",
    "        max_h = max(mask.shape[0] for mask in masks)\n",
    "        max_w = max(mask.shape[1] for mask in masks)\n",
    "        \n",
    "        # Pad masks to the same size\n",
    "        padded_masks = []\n",
    "        for mask in masks:\n",
    "            h, w = mask.shape\n",
    "            padded = np.zeros((max_h, max_w), dtype=mask.dtype)\n",
    "            padded[:h, :w] = mask\n",
    "            padded_masks.append(torch.as_tensor(padded))\n",
    "        \n",
    "        grouped_data[\"ground_truth_mask\"] = torch.stack(padded_masks)\n",
    "    \n",
    "    # Process tensor data\n",
    "    for key in list(grouped_data.keys()):\n",
    "        if key == \"ground_truth_mask\":\n",
    "            continue  # Already processed\n",
    "        elif isinstance(grouped_data[key][0], torch.Tensor):\n",
    "            # Handle tensor data (from processor)\n",
    "            # For tensors with same shape, stack them\n",
    "            try:\n",
    "                grouped_data[key] = torch.stack(grouped_data[key])\n",
    "            except:\n",
    "                # For tensors with different shapes, use appropriate method\n",
    "                if len(grouped_data[key][0].shape) <= 1:  # 1D tensors\n",
    "                    grouped_data[key] = pad_sequence(grouped_data[key], batch_first=True)\n",
    "                else:\n",
    "                    # For tensors with different shapes that can't be stacked,\n",
    "                    # create a dict with \"_batch_idx\" suffix to maintain the batch structure\n",
    "                    for i, tensor in enumerate(grouped_data[key]):\n",
    "                        grouped_data[f\"{key}_batch_idx_{i}\"] = tensor\n",
    "                    # Remove the original key which is a list\n",
    "                    del grouped_data[key]\n",
    "        elif isinstance(grouped_data[key][0], (str, tuple)):\n",
    "            # For strings and tuples, create a new entry with batch index\n",
    "            for i, item in enumerate(grouped_data[key]):\n",
    "                grouped_data[f\"{key}_batch_idx_{i}\"] = item\n",
    "            del grouped_data[key]\n",
    "        elif isinstance(grouped_data[key][0], list):\n",
    "            # For lists, convert to tensors if they contain numbers\n",
    "            try:\n",
    "                # Try to convert to a tensor\n",
    "                grouped_data[key] = torch.tensor(grouped_data[key])\n",
    "            except:\n",
    "                # If it fails, create individual entries\n",
    "                for i, item in enumerate(grouped_data[key]):\n",
    "                    grouped_data[f\"{key}_batch_idx_{i}\"] = torch.tensor(item) if isinstance(item[0], (int, float)) else item\n",
    "                del grouped_data[key]\n",
    "        else:\n",
    "            # Try to convert to tensor if possible, otherwise create individual entries\n",
    "            try:\n",
    "                grouped_data[key] = torch.tensor(grouped_data[key])\n",
    "            except:\n",
    "                for i, item in enumerate(grouped_data[key]):\n",
    "                    grouped_data[f\"{key}_batch_idx_{i}\"] = item\n",
    "                del grouped_data[key]\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "# Example usage:\n",
    "def load_sam2_datasets(training_path, instances_path, processor, batch_size=2, resize=None):\n",
    "    \"\"\"\n",
    "    Load train and validation datasets formatted for SAM2.\n",
    "    \n",
    "    Args:\n",
    "        training_path (str): Path to training images\n",
    "        instances_path (str): Path to instance masks\n",
    "        processor: SAM2 processor for input preparation\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        resize (tuple, optional): If provided, resize all images to this size (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader: DataLoaders for train and validation sets\n",
    "    \"\"\"\n",
    "    # Create base datasets\n",
    "    train_base = KITTIMOTSBaseDataset(\n",
    "        training_path=training_path,\n",
    "        instances_path=instances_path,\n",
    "        split=\"train\",\n",
    "        resize=resize\n",
    "    )\n",
    "    \n",
    "    val_base = KITTIMOTSBaseDataset(\n",
    "        training_path=training_path,\n",
    "        instances_path=instances_path,\n",
    "        split=\"val\",\n",
    "        resize=resize\n",
    "    )\n",
    "    \n",
    "    # Wrap with SAM2 dataset\n",
    "    train_dataset = SAMDataset(dataset=train_base, processor=processor)\n",
    "    val_dataset = SAMDataset(dataset=val_base, processor=processor)\n",
    "    \n",
    "    # Create data loaders with custom collate function if resize is None\n",
    "    if resize is None:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "    else:\n",
    "        # If using fixed resize, standard collate function is fine\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "train_loader, val_loader = load_sam2_datasets(TRAINING_PATH, INSTANCES_PATH, processor, resize=(1024, 1024), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch[\"ground_truth_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                 | 0/5027 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 9.63 GiB of which 291.69 MiB is free. Including non-PyTorch memory, this process has 5.25 GiB memory in use. Of the allocated memory 4.90 GiB is allocated by PyTorch, and 102.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m epoch_losses = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[32m     16\u001b[39m   \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m   outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                  \u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_boxes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m   \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[32m     22\u001b[39m   predicted_masks = outputs.pred_masks.squeeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/transformers/models/sam/modeling_sam.py:1507\u001b[39m, in \u001b[36mSamModel.forward\u001b[39m\u001b[34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1504\u001b[39m vision_hidden_states = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m     vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1513\u001b[39m     image_embeddings = vision_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/transformers/models/sam/modeling_sam.py:1193\u001b[39m, in \u001b[36mSamVisionEncoder.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1188\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1189\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1190\u001b[39m         hidden_states,\n\u001b[32m   1191\u001b[39m     )\n\u001b[32m   1192\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/transformers/models/sam/modeling_sam.py:1086\u001b[39m, in \u001b[36mSamVisionLayer.forward\u001b[39m\u001b[34m(self, hidden_states, output_attentions)\u001b[39m\n\u001b[32m   1083\u001b[39m     height, width = hidden_states.shape[\u001b[32m1\u001b[39m], hidden_states.shape[\u001b[32m2\u001b[39m]\n\u001b[32m   1084\u001b[39m     hidden_states, padding_shape = \u001b[38;5;28mself\u001b[39m.window_partition(hidden_states, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mcv-c5-team1/lib/python3.12/site-packages/transformers/models/sam/modeling_sam.py:879\u001b[39m, in \u001b[36mSamVisionAttention.forward\u001b[39m\u001b[34m(self, hidden_states, output_attentions)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# q, k, v with shape (batch_size * nHead, height * width, channel)\u001b[39;00m\n\u001b[32m    877\u001b[39m query, key, value = qkv.reshape(\u001b[32m3\u001b[39m, batch_size * \u001b[38;5;28mself\u001b[39m.num_attention_heads, height * width, -\u001b[32m1\u001b[39m).unbind(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m attn_weights = \u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_rel_pos:\n\u001b[32m    882\u001b[39m     attn_weights = \u001b[38;5;28mself\u001b[39m.add_decomposed_rel_pos(\n\u001b[32m    883\u001b[39m         attn_weights, query, \u001b[38;5;28mself\u001b[39m.rel_pos_h, \u001b[38;5;28mself\u001b[39m.rel_pos_w, (height, width), (height, width)\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 9.63 GiB of which 291.69 MiB is free. Including non-PyTorch memory, this process has 5.25 GiB memory in use. Of the allocated memory 4.90 GiB is allocated by PyTorch, and 102.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                      multimask_output=False)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
